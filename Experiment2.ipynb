{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libaries \n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\Uni-CE901\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reading txt files\n",
    "\n",
    "# define what documents to load and where from\n",
    "class UTF8TextLoader(TextLoader):\n",
    "    def __init__(self, file_path: str, encoding: str = 'utf-8'):\n",
    "        super().__init__(file_path, encoding)\n",
    "        \n",
    "loader = DirectoryLoader(\"./\", glob=\"*.txt\", loader_cls=UTF8TextLoader)\n",
    "\n",
    "# interpret information in the documents provided\n",
    "documents = loader.load()\n",
    "splitter = CharacterTextSplitter()\n",
    "texts = splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# create and save the local database\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(\"faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1231 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Anxiety_Causes.txt, Tokens: 271\n",
      "File: Anxiety_HelpingFriends.txt, Tokens: 950\n",
      "WARNING: Anxiety_HelpingFriends.txt exceeds the maximum token limit of 512\n",
      "File: Anxiety_PanicAttacks.txt, Tokens: 117\n",
      "File: Anxiety_symptoms.txt, Tokens: 204\n",
      "File: Anxiety_WhatToDo.txt, Tokens: 262\n",
      "File: Anxiety_WhatToSay.txt, Tokens: 399\n",
      "File: Depression.txt, Tokens: 1231\n",
      "WARNING: Depression.txt exceeds the maximum token limit of 512\n",
      "File: Depression_Causes.txt, Tokens: 774\n",
      "WARNING: Depression_Causes.txt exceeds the maximum token limit of 512\n",
      "File: Depression_HelpingFriends.txt, Tokens: 767\n",
      "WARNING: Depression_HelpingFriends.txt exceeds the maximum token limit of 512\n",
      "File: Depression_HowToTell&Living.txt, Tokens: 213\n",
      "File: Depression_Symptoms.txt, Tokens: 271\n",
      "File: Depression_Treatment&Doctor.txt, Tokens: 207\n",
      "File: Depression_Treatments.txt, Tokens: 1231\n",
      "WARNING: Depression_Treatments.txt exceeds the maximum token limit of 512\n",
      "File: Depression_Types.txt, Tokens: 519\n",
      "WARNING: Depression_Types.txt exceeds the maximum token limit of 512\n",
      "File: Depression_WhatToSay.txt, Tokens: 512\n",
      "File: Depression_WhenToSeeDoctor&Treatments.txt, Tokens: 207\n",
      "File: Depression_WhenToSeeDoctor.txt, Tokens: 207\n",
      "File: Stress_Causes.txt, Tokens: 373\n",
      "File: Stress_HelpingFriends.txt, Tokens: 491\n",
      "File: Stress_Symptoms.txt, Tokens: 427\n",
      "File: Stress_WhatIsIt.txt, Tokens: 128\n",
      "File: Stress_WhatToDo.txt, Tokens: 770\n",
      "WARNING: Stress_WhatToDo.txt exceeds the maximum token limit of 512\n"
     ]
    }
   ],
   "source": [
    "# Set the path to where you have the text files\n",
    "directory_path = r\"Z:\\Uni-CE901\\22-24_CE901-CE911-CF981-SU_stoyles_thomas\\Data\"  # Change this to your directory\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # Change if you wish to test other tokenizers\n",
    "\n",
    "# Counting tokens in files and directory\n",
    "def count_tokens_in_file(file_path, tokenizer):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def check_tokens_in_directory(directory_path, tokenizer, max_tokens=512):\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            num_tokens = count_tokens_in_file(file_path, tokenizer)\n",
    "            print(f\"File: {file_name}, Tokens: {num_tokens}\")\n",
    "            if num_tokens > max_tokens:\n",
    "                print(f\"WARNING: {file_name} exceeds the maximum token limit of {max_tokens}\")\n",
    "\n",
    "# Check token counts in the directory\n",
    "check_tokens_in_directory(directory_path, tokenizer, max_tokens=512) # Adjust max_tokens to your max token count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\Uni-CE901\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# prepare the template we will use when prompting the response\n",
    "template = \"\"\"Context: {context} Question: {question}\"\"\"\n",
    "\n",
    "# load the language model \n",
    "config = {'max_new_tokens': 512, 'temperature': 0.01}\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Enable dangerous deserialization\n",
    "if hasattr(llm, 'allow_dangerous_deserialization'):\n",
    "    llm.allow_dangerous_deserialization = True\n",
    "\n",
    "# load the interpreted information from the local database\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 5}) # Max number of documents to read (k)\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'])\n",
    "\n",
    "QA_LLM = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={'prompt': prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_tokens(text, max_tokens, tokenizer):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens[:max_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining query\n",
    "\n",
    "def query(model, question):\n",
    "    model_path = model.combine_documents_chain.llm_chain.llm.model\n",
    "    model_name = pathlib.Path(model_path).name\n",
    "    time_start = time.time()\n",
    "    output = model({'query': question})\n",
    "    response = output[\"result\"]\n",
    "    time_elapsed = time.time() - time_start\n",
    "\n",
    "    # Extract and display the source document\n",
    "    source_document = output['source_documents'][0].page_content\n",
    "\n",
    "    # Display information in this order \n",
    "    display(HTML(f'<code>{model_name} response time: {time_elapsed:.02f} sec</code>'))\n",
    "    display(HTML(f'<strong>Question:</strong> {question}'))\n",
    "    display(HTML(f'<strong>Answer:</strong> {response}'))\n",
    "    display(HTML(f'<strong>Source Document:</strong> <pre>{source_document}</pre>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<code>llama3 response time: 8.00 sec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> What causes Anxiety? "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> Based on the provided context and prompt, I will attempt to provide a response that addresses the question \"What causes anxiety?\"\n",
       "\n",
       "To answer this question, I will utilize the Ollama language model. First, let's set up the experiment by running the code in Experiment1.ipynb.\n",
       "\n",
       "We can use the query function to input our question and retrieve a response from the model. The response should provide information on what causes anxiety.\n",
       "\n",
       "Here's the prompt template:\n",
       "\"A [prompt] is defined to structure the input to the language model, enhancing the quality of the generated responses.\"\n",
       "\n",
       "In this case, the prompt could be \"What are some common causes of anxiety?\"\n",
       "\n",
       "The Ollama language model can then generate a response based on the information it has learned from the text documents and the RAG system implemented.\n",
       "\n",
       "Let's run the experiment and see what the model has to say!\n",
       "\n",
       "Please note that the actual response will depend on the specific Ollama model used, as well as any additional preprocessing steps or fine-tuning of the prompt structure."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Source Document:</strong> <pre>Prompt Template:\n",
       "A prompt template is defined to structure the input to the language model, enhancing the quality of the generated responses.\n",
       "\n",
       "Query Function:\n",
       "A token truncation function is introduced to ensure that inputs to the language model are within the allowed token limit.\n",
       "\n",
       "Editing the model \n",
       "To edit the model you will first need to install the Ollama model you wish to use. You can do this by using the Ollama pull function as stated above. You will also need to change the loader \n",
       "variable of the notebook, currently it reading from the defult loading area of the notebook however if you run into any issues you will need to change the location of the variable to where \n",
       "your have stored the text files. You will also need to change the directory_path variable to your directory.\n",
       "\n",
       "\n",
       "Experiment3_AllDocsPrint.ipynb\n",
       "\n",
       "Overview\n",
       "Experiment3_AllDocsPrint.ipynb is used to print all the documents that the models read to make sure that the program is correctly reading infomation from multiple text files. This allows us \n",
       "to test questions that will have infomation in more than one text file. \n",
       "\n",
       "\n",
       "Experiment3_chunkTest.ipynb\n",
       "\n",
       "Overview\n",
       "Experiment3_chunkTest.ipynb similar to Experiment3_AllDocsPrint.ipynb is used to print all the chunks that the models read to make sure that the program is correctly following the RAG system \n",
       "implemented and that the chunks are loading corrcetly \n",
       "\n",
       "\n",
       "Experiment3_Tokens.ipynb\n",
       "\n",
       "Overview\n",
       "Experiment3_Tokens.ipynb is used to test the model under different token amounts. This allowed me to experiment with the token amount that the model had access to, checking if \n",
       "the token amount would change the response given to the model\n",
       "\n",
       "\n",
       "Experiment3_Comparision.ipynb\n",
       "\n",
       "Overview\n",
       "Experiment3_Comparision.ipynb is used to test llama3 and llama3.1, this notebook shows the user 2 responses without knowing which model is what. You then choosde the best response to the\n",
       "question to reveal the model which has given the better response.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question asked\n",
    "query(QA_LLM, \"What causes Anxiety? \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
